{
    "$schema": "https://raw.githubusercontent.com/Microsoft/azure-pipelines-task-lib/master/tasks.schema.json",
    "id": "7c8f9a2b-1d3e-4f5a-9b7c-8e2d1f4a6c9b",
    "name": "OllamaCodeReview",
    "friendlyName": "Ollama Code Review",
    "description": "Complete a Code Review using on-premises hosted Ollama",
    "category": "Utility",
    "author": "teriansilva",
    "version": {
        "Major": 2,
        "Minor": 4,
        "Patch": 7
    },
    "instanceNameFormat": "Ollama Code Review $(message)",
    "inputs": [
        {
            "name": "ollama_endpoint",
            "type": "string",
            "label": "Ollama API Endpoint",
            "defaultValue": "http://localhost:11434/api/chat",
            "required": true,
            "helpMarkDown": "The full URL to your Ollama API endpoint. Supports both HTTP and HTTPS. Example: http://localhost:11434/api/chat or https://ollama.yourdomain.com/api/chat"
        },
        {
            "name": "ai_model",
            "type": "string",
            "label": "Ollama Model",
            "defaultValue": "gpt-oss:20b",
            "required": true,
            "helpMarkDown": "## Ollama Model Configuration\n\n### Description\nSpecify the Ollama model to use for code analysis. You can use any model available in your Ollama instance.\n\n### Recommended Models for Code Review\n- **gpt-oss** - Excellent for code review with strong reasoning (Highly Recommended)\n- **qwen2.5-coder** - Advanced code analysis and understanding\n- **deepseek-coder-v2** - Latest version optimized for code understanding\n- **codellama** - Meta's Code Llama model (stable and reliable)\n- **llama3.3** - Latest Llama model with improved reasoning\n- **llama3.2** - Llama 3.2 with strong performance\n- **mistral-large** - Large general-purpose model with excellent capabilities\n- **mixtral** - Mixture of experts model with strong performance\n- **codegemma** - Google's code-focused model\n\n### Custom Models\nYou can enter any custom model name that exists in your Ollama instance. Run `ollama list` on your Ollama server to see available models.\n\n### Default Value\nThe default model is set to **codellama** for compatibility.\n\nChoose or enter the model name that best suits your code review requirements."
        },
        {
            "name": "bugs",
            "type": "boolean",
            "label": "Check for bugs",
            "defaultValue": true,
            "helpMarkDown": "Specify whether to enable bug checking during the code review process.\n\n- Set to `true` to perform bug checks.\n- Set to `false` to skip bug checks.\n\nBug checking helps identify and address potential issues in the code. Default value is `true`."
        },
        {
            "name": "performance",
            "type": "boolean",
            "label": "Check for performance problems",
            "defaultValue": true,
            "helpMarkDown": "Specify whether to include performance checks during the code review process.\n\n- Set to `true` to perform performance checks.\n- Set to `false` to skip performance checks.\n\nEnabling performance checks helps identify and address potential performance-related issues in the code. Default value is `true`."
        },
        {
            "name": "best_practices",
            "type": "boolean",
            "label": "Check for missed best practices",
            "defaultValue": true,
            "helpMarkDown": "Specify whether to include checks for missed best practices during the code review process.\n\n- Set to `true` to perform best practices checks.\n- Set to `false` to skip best practices checks.\n\nEnabling best practices checks helps ensure adherence to coding standards and identifies areas for improvement. Default value is `true`."
        },
        {
            "name": "file_extensions",
            "type": "string",
            "label": "File Extensions",
            "defaultValue": "",
            "required": false,
            "helpMarkDown": "Specify a comma-separated list of file extensions for which you want to perform a code review. This input helps narrow down the scope of the code review to specific file types.\n\n**Example:**\n```plaintext\n\".js,.ts,.css,.html\"\n```\n\nMake sure to provide the file extensions without spaces after the commas."
        },
        {
            "name": "file_excludes",
            "type": "string",
            "label": "Files to exclude",
            "defaultValue": "",
            "required": false,
            "helpMarkDown": "## Files to Exclude Configuration\n\n### Description\nSpecify a comma-separated list of file names that should be excluded from code reviews. This is useful for excluding sensitive files or preventing certain files from being reviewed.\n\n### Format\nProvide a list of file names separated by commas. For example: `file1.js, file2.py, secret.txt`\n\n### Default Value\nIf no files are specified, all files will be considered for code review by default."
        },
        {
            "name": "additional_prompts",
            "type": "string",
            "label": "Additional Prompts",
            "defaultValue": "",
            "required": false,
            "helpMarkDown": "Specify additional Open AI prompts as a comma-separated list to enhance the code review process.\n\n- Add multiple prompts separated by commas.\n- These prompts will be used in conjunction with the main code review prompts.\n\n**Example:**\n```plaintext\nFix variable naming, Ensure consistent indentation, Review error handling approach\n```"
        },
        {
            "name": "bearer_token",
            "type": "string",
            "label": "Bearer Token (Optional)",
            "defaultValue": "",
            "required": false,
            "helpMarkDown": "## Bearer Token Configuration\n\n### Description\nProvide the Bearer token to authenticate with your secured Ollama API endpoint. This token will be sent in the `Authorization: Bearer <token>` header.\n\n**Leave empty if your Ollama endpoint does not require authentication.**\n\n### When to Use\nUse this when your Ollama API is exposed through a reverse proxy (like nginx) that requires Bearer token authentication.\n\n### Security Note\nStore sensitive tokens as pipeline variables or Azure Key Vault secrets rather than hardcoding them.\n\n### Example\nUse `$(OllamaApiToken)` to reference a pipeline variable."
        },
        {
            "name": "custom_best_practices",
            "type": "multiLine",
            "label": "Custom Best Practices",
            "defaultValue": "",
            "required": false,
            "helpMarkDown": "## Custom Best Practices Configuration\n\n### Description\nDefine project-specific or team-specific best practices that the AI should check for during code review. This helps enforce your organization's coding standards and conventions.\n\n### Format\nEnter each best practice on a new line or as a paragraph. The AI will use these guidelines when reviewing code.\n\n### Examples\n- Always use async/await instead of .then() for promises\n- All public methods must have JSDoc comments\n- Database queries must use parameterized statements\n- Error messages must be logged with context\n- CSS class names must follow BEM methodology\n\n### Default Value\nIf left empty, only the standard checks (bugs, performance, best practices) will be applied."
        },
        {
            "name": "include_build_logs",
            "type": "boolean",
            "label": "Include Build Log Context",
            "defaultValue": false,
            "required": false,
            "helpMarkDown": "## Include Build Log Context\n\n### Description\nWhen enabled, the AI model will receive context from the build logs of previous pipeline steps. This helps the model understand:\n\n- Whether the project builds successfully\n- Any compilation errors or warnings\n- Test results from previous steps\n- Other relevant build output\n\n### Use Cases\n- When you want the AI to consider build context in its review\n- To help identify if code changes might cause build issues\n- To provide context about the project's build health\n\n### Performance Note\nEnabling this option may increase the token count sent to the model. Build logs are truncated to prevent exceeding token limits.\n\n### Default Value\nDisabled by default to optimize performance."
        },
        {
            "name": "build_log_tasks",
            "type": "string",
            "label": "Build Log Task Filter",
            "defaultValue": "",
            "required": false,
            "helpMarkDown": "## Build Log Task Filter\n\n### Description\nSpecify which pipeline tasks to include when gathering build logs. Only logs from matching tasks will be included.\n\n### Format\nComma-separated list of task display names (partial matches supported). Leave empty to include all tasks.\n\n### Examples\n- `Build,Test` - Include only logs from tasks containing 'Build' or 'Test'\n- `DotNetCoreCLI,VSTest` - Include .NET build and test task logs\n- `npm,webpack` - Include npm and webpack task logs\n\n### Default Value\nEmpty (include all task logs)"
        },
        {
            "name": "include_pr_comments",
            "type": "boolean",
            "label": "Include Existing PR Comments",
            "defaultValue": false,
            "required": false,
            "helpMarkDown": "## Include Existing PR Comments\n\n### Description\nWhen enabled, the AI model will receive context from existing human-written comments on the Pull Request. This helps the model:\n\n- Understand ongoing discussions about the code\n- Avoid duplicating points already raised by reviewers\n- Provide more contextually relevant feedback\n- Build upon existing review conversations\n\n### What's Included\n- Comments from human reviewers (not AI-generated comments)\n- File-specific and general PR comments\n- Thread status (Active, Fixed, Closed, etc.)\n\n### What's Excluded\n- AI-generated comments from previous runs (to avoid feedback loops)\n- Deleted comments\n- System-generated messages\n\n### Performance Note\nEnabling this option may increase the token count sent to the model.\n\n### Default Value\nDisabled by default to optimize performance."
        },
        {
            "name": "token_limit",
            "type": "string",
            "label": "Token Limit",
            "defaultValue": "8192",
            "required": false,
            "helpMarkDown": "## Token Limit Configuration\n\n### Description\nSpecify the maximum number of tokens that can be sent to the AI model in a single request. This limit includes the system prompt, file content, diff, and project context.\n\n### How It Works\n- If the full context (file + diff + project metadata) exceeds this limit, the task will attempt to review with just the diff\n- If even the diff exceeds this limit, the file will be skipped with a warning\n\n### Recommended Values\n- **8192** - Default, works well for most models\n- **16384** - For models with larger context windows (e.g., llama3.2)\n- **32768** - For models with very large context windows (e.g., qwen2.5-coder:32k)\n- **65536** - For models with extended context (e.g., deepseek-coder-v2)\n- **131072** - For models with 128k context windows\n\n### Note\nSet this value based on your model's actual context window size. Setting it too high for a model with a smaller context window will result in API errors.\n\n### Default Value\n8192 tokens (conservative default for broad compatibility)"
        },
        {
            "name": "max_file_content_tokens",
            "type": "string",
            "label": "Max File Content Tokens",
            "defaultValue": "4000",
            "required": false,
            "helpMarkDown": "## Max File Content Tokens\n\n### Description\nMaximum number of tokens to include from the full file content. If a file exceeds this limit, only the first portion will be included, and the AI will rely more on the diff.\n\n### Recommended Values\n- **2000** - Conservative, leaves more room for diff and context\n- **4000** - Default, good balance\n- **8000** - For larger context windows\n- **0** - Disable full file content entirely (diff-only mode)\n\n### Default Value\n4000 tokens"
        },
        {
            "name": "max_project_context_tokens",
            "type": "string",
            "label": "Max Project Context Tokens",
            "defaultValue": "2000",
            "required": false,
            "helpMarkDown": "## Max Project Context Tokens\n\n### Description\nMaximum number of tokens to include from project context (README, package.json, .csproj files, etc.). If project context exceeds this limit, it will be truncated.\n\n### Recommended Values\n- **1000** - Minimal context\n- **2000** - Default, includes key project info\n- **4000** - More detailed project context\n- **0** - Disable project context entirely\n\n### Default Value\n2000 tokens"
        },
        {
            "name": "custom_system_prompt",
            "type": "multiLine",
            "label": "Custom System Prompt (Advanced)",
            "defaultValue": "",
            "required": false,
            "helpMarkDown": "## Custom System Prompt\n\n### Description\nCompletely override the default system prompt with your own instructions. This is an advanced option for users who want full control over how the AI behaves.\n\n### Warning\n**If you provide a custom system prompt, you MUST include instructions for the AI to respond in the correct JSON format**, otherwise comments will not be posted correctly.\n\n### Required JSON Format\nYour prompt must instruct the AI to respond with:\n```json\n{\n  \"comments\": [\n    {\n      \"lineNumber\": <number>,\n      \"comment\": \"<string>\"\n    }\n  ]\n}\n```\n\n### Default Value\nEmpty (uses the built-in system prompt)"
        }
    ],
    "execution": {
        "Node20_1": {
            "target": "main.js"
        }
    }
}